{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install optuna"
      ],
      "metadata": {
        "id": "JklxzOZ09_6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbuQ2Z6_HuMQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import optuna\n",
        "from optuna.samplers import TPESampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dailyData = pd.read_csv(\"drive/MyDrive/Adidas Sales/sales_simplified.csv\")\n",
        "dailyData.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
        "dailyData.sort_values(by=\"index\", inplace=True)\n",
        "dailyData[\"index\"] = pd.to_datetime(dailyData[\"index\"])\n",
        "weeklyData = pd.DataFrame()\n",
        "for product in dailyData[\"Product\"].unique():\n",
        "  buffer = dailyData[dailyData[\"Product\"] == product].copy()\n",
        "  buffer.sort_values(by=\"index\", inplace=True)\n",
        "  buffer = buffer.resample(\"W\", kind=\"timestamp\", on=\"index\").mean()\n",
        "  buffer[\"Product\"] = product\n",
        "  buffer.reset_index(level=\"index\", inplace=True)\n",
        "  if len(weeklyData) == 0:\n",
        "    weeklyData = buffer\n",
        "  else:\n",
        "    weeklyData = pd.concat([weeklyData, buffer])"
      ],
      "metadata": {
        "id": "7Es9KEy6q6PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dailyTreshold = \"2021-11-01\"\n",
        "weeklyTreshold = \"2021-10-24\""
      ],
      "metadata": {
        "id": "0kucFtwwrTKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dailyData_train, dailyData_test = dailyData.loc[dailyData[\"index\"] <= pd.Timestamp(dailyTreshold)].copy(), dailyData.loc[dailyData[\"index\"] > pd.Timestamp(dailyTreshold)].copy()\n",
        "\n",
        "weeklyData_train, weeklyData_test = weeklyData.loc[weeklyData[\"index\"] <= pd.Timestamp(weeklyTreshold)].copy(), weeklyData.loc[weeklyData[\"index\"] > pd.Timestamp(weeklyTreshold)].copy()"
      ],
      "metadata": {
        "id": "9wbHDam4rTMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Согласно https://www.tensorflow.org/tutorials/structured_data/time_series?hl=ru данные перед подачей в нейронную сеть надо нормализовать"
      ],
      "metadata": {
        "id": "4ZhOBmMUrnZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dailyStats, weeklyStats = {}, {}\n",
        "dailyData_train_clear, weeklyData_train_clear = pd.DataFrame(), pd.DataFrame()\n",
        "dailyData_test_clear, weeklyData_test_clear = pd.DataFrame(), pd.DataFrame()\n",
        "for product in dailyData_train[\"Product\"].unique():\n",
        "  bufferDaily = dailyData_train[dailyData_train[\"Product\"] == product].copy()\n",
        "  bufferWeekly = weeklyData_train[weeklyData_train[\"Product\"] == product].copy()\n",
        "\n",
        "  dailyStats[product], weeklyStats[product] = {}, {}\n",
        "\n",
        "  dailyStats[product][\"mean\"], dailyStats[product][\"std\"] = bufferDaily[\"Units Sold\"].mean(), bufferDaily[\"Units Sold\"].std()\n",
        "  weeklyStats[product][\"mean\"], weeklyStats[product][\"std\"] = bufferWeekly[\"Units Sold\"].mean(), bufferWeekly[\"Units Sold\"].std()\n",
        "\n",
        "  bufferDaily[\"Units Sold\"] = (bufferDaily[\"Units Sold\"] - dailyStats[product][\"mean\"])/dailyStats[product][\"std\"]\n",
        "  bufferWeekly[\"Units Sold\"] = (bufferWeekly[\"Units Sold\"] - weeklyStats[product][\"mean\"])/weeklyStats[product][\"std\"]\n",
        "\n",
        "  if len(dailyData_train_clear) == 0:\n",
        "    dailyData_train_clear = bufferDaily\n",
        "    weeklyData_train_clear = bufferWeekly\n",
        "  else:\n",
        "    dailyData_train_clear = pd.concat([dailyData_train_clear, bufferDaily])\n",
        "    weeklyData_train_clear = pd.concat([weeklyData_train_clear, bufferWeekly])\n",
        "  \n",
        "  bufferDaily = dailyData_test[dailyData_test[\"Product\"] == product].copy()\n",
        "  bufferWeekly = weeklyData_test[weeklyData_test[\"Product\"] == product].copy()\n",
        "\n",
        "  bufferDaily[\"Units Sold\"] = (bufferDaily[\"Units Sold\"] - dailyStats[product][\"mean\"])/dailyStats[product][\"std\"]\n",
        "  bufferWeekly[\"Units Sold\"] = (bufferWeekly[\"Units Sold\"] - weeklyStats[product][\"mean\"])/weeklyStats[product][\"std\"]\n",
        "\n",
        "  if len(dailyData_train_clear) == 0:\n",
        "    dailyData_test_clear = bufferDaily\n",
        "    weeklyData_test_clear = bufferWeekly\n",
        "  else:\n",
        "    dailyData_test_clear = pd.concat([dailyData_test_clear, bufferDaily])\n",
        "    weeklyData_test_clear = pd.concat([weeklyData_test_clear, bufferWeekly])"
      ],
      "metadata": {
        "id": "sw2hTciVAM_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По аналогии с https://www.tensorflow.org/tutorials/structured_data/time_series?hl=ru будем обучать модели на прогнозирование значений на основе окна последовательных выборок из данных."
      ],
      "metadata": {
        "id": "gbjGRadx8Auu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WindowGenerator():\n",
        "  def __init__(self, input_width, label_width, shift, train_df=None, test_df=None, label_columns=None):\n",
        "    # Store the raw data.\n",
        "    self.train_df = train_df\n",
        "    self.test_df = test_df\n",
        "\n",
        "    # Work out the label column indices.\n",
        "    self.label_columns = label_columns\n",
        "    if label_columns is not None:\n",
        "      self.label_columns_indices = {name: i for i, name in\n",
        "                                    enumerate(label_columns)}\n",
        "    self.column_indices = {name: i for i, name in\n",
        "                           enumerate(train_df.columns)}\n",
        "\n",
        "    # Work out the window parameters.\n",
        "    self.input_width = input_width\n",
        "    self.label_width = label_width\n",
        "    self.shift = shift\n",
        "\n",
        "    self.total_window_size = input_width + shift\n",
        "\n",
        "    self.input_slice = slice(0, input_width)\n",
        "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "\n",
        "    self.label_start = self.total_window_size - self.label_width\n",
        "    self.labels_slice = slice(self.label_start, None)\n",
        "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "  def __repr__(self):\n",
        "    return '\\n'.join([\n",
        "        f'Total window size: {self.total_window_size}',\n",
        "        f'Input indices: {self.input_indices}',\n",
        "        f'Label indices: {self.label_indices}',\n",
        "        f'Label column name(s): {self.label_columns}'])\n",
        "  \n",
        "  def split_window(self, features):\n",
        "    inputs = features[:, self.input_slice, :]\n",
        "    labels = features[:, self.labels_slice, :]\n",
        "    if self.label_columns is not None:\n",
        "      labels = tf.stack(\n",
        "          [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "          axis=-1)\n",
        "\n",
        "    # Slicing doesn't preserve static shape information, so set the shapes\n",
        "    # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "    inputs.set_shape([None, self.input_width, None])\n",
        "    labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "    return inputs, labels\n",
        "\n",
        "  def make_dataset(self, data):\n",
        "    data = np.array(data, dtype=np.float32)\n",
        "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
        "        data=data,\n",
        "        targets=None,\n",
        "        sequence_length=self.total_window_size,\n",
        "        sequence_stride=1,\n",
        "        shuffle=True,\n",
        "        batch_size=32)\n",
        "\n",
        "    ds = ds.map(self.split_window)\n",
        "\n",
        "    return ds\n",
        "  \n",
        "  @property\n",
        "  def train(self):\n",
        "    return self.make_dataset(self.train_df)\n",
        "\n",
        "  @property\n",
        "  def test(self):\n",
        "    return self.make_dataset(self.test_df)"
      ],
      "metadata": {
        "id": "xpb54vOmsISW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_EPOCHS = 20"
      ],
      "metadata": {
        "id": "qIdLJ--4xGOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compile_and_fit(model, window, patience=2):\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience, mode='min')\n",
        "\n",
        "  model.compile(loss=tf.losses.MeanSquaredError(), optimizer=tf.optimizers.Adam())\n",
        "\n",
        "  history = model.fit(window.train, epochs=MAX_EPOCHS, callbacks=[early_stopping])\n",
        "  return history"
      ],
      "metadata": {
        "id": "VJNWETIZxGpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_model(num_filters, label_width, conv_width, num_features, activation):\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, conv_width, features]\n",
        "    tf.keras.layers.Lambda(lambda x: x[:, -conv_width:, :]),\n",
        "    # Shape => [batch, 1, conv_units]\n",
        "    tf.keras.layers.Conv1D(num_filters, activation=activation, kernel_size=(conv_width)),\n",
        "    # Shape => [batch, 1,  out_steps*features]\n",
        "    tf.keras.layers.Dense(label_width*num_features, kernel_initializer=tf.initializers.zeros()),\n",
        "    # Shape => [batch, out_steps, features]\n",
        "    tf.keras.layers.Reshape([label_width, num_features])\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "O543XdnISZA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для начала напишем CNN сети для одномерного случая"
      ],
      "metadata": {
        "id": "Pt75hSAzc__A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dailyData_train)/6, len(dailyData_test)/6"
      ],
      "metadata": {
        "id": "PwQe7yaEWYA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dailyTrainBuffer, dailyTestBuffer = pd.DataFrame(), pd.DataFrame()"
      ],
      "metadata": {
        "id": "JgcW4DfuZJ7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сверточная нейронная сеть\n",
        "def objectiveDaily1DConv(trial):\n",
        "  #Горизонт прогнозирования в окне - 3 недели\n",
        "  label_width = 21\n",
        "  conv_width = trial.suggest_int('conv_width', 3, label_width)\n",
        "  # Длинна тестового датасета 60: окно не должно быть длинее!!!\n",
        "  input_width = label_width + conv_width - 3\n",
        "  num_filters = trial.suggest_int('num_filters', input_width, 3*input_width)\n",
        "\n",
        "  conv_model = cnn_model(num_filters, label_width, conv_width, 1, trial.suggest_categorical('activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh']))\n",
        "\n",
        "  multi_window = WindowGenerator(input_width=input_width,\n",
        "                               label_width=label_width,\n",
        "                               shift=label_width, \n",
        "                               train_df=dailyTrainBuffer, \n",
        "                               test_df=dailyTestBuffer, \n",
        "                               label_columns=[\"Units Sold\"])\n",
        "\n",
        "  history = compile_and_fit(conv_model, multi_window)\n",
        "\n",
        "  loss = conv_model.evaluate(multi_window.test, verbose=0, return_dict=True)\n",
        "\n",
        "  return math.sqrt(conv_model.evaluate(multi_window.test, verbose=0, return_dict=True)[\"loss\"])"
      ],
      "metadata": {
        "id": "ewfA_PD-y0x1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily1DConvModels = {}\n",
        "train_time = 3600\n",
        "for product in dailyData_train_clear[\"Product\"].unique():\n",
        "  dailyTrainBuffer = dailyData_train_clear[dailyData_train_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  dailyTestBuffer = dailyData_test_clear[dailyData_test_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='1D_CNN_Daily')\n",
        "  study.optimize(objectiveDaily1DConv, timeout=train_time, show_progress_bar=True)\n",
        "  trial = study.best_trial\n",
        "  daily1DConvModels[product] = trial.params"
      ],
      "metadata": {
        "id": "8ls7WSq6M9oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(daily1DConvModels, index=daily1DConvModels[product].keys())"
      ],
      "metadata": {
        "id": "ImxLKyZmYRPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(weeklyData_train)/6, len(weeklyData_test)/6"
      ],
      "metadata": {
        "id": "z-NqpoA3WWBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weeklyTrainBuffer, weeklyTestBuffer = pd.DataFrame(), pd.DataFrame()"
      ],
      "metadata": {
        "id": "szEgVZULXCGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objectiveWeekly1DConv(trial):\n",
        "  #Горизонт прогнозирования в окне - 3 недели\n",
        "  label_width = 3\n",
        "  conv_width = trial.suggest_int('conv_width', 3, label_width)\n",
        "  # Длинна тестового датасета 10: окно не должно быть длинее!!!\n",
        "  input_width = label_width + conv_width - 1\n",
        "  num_filters = trial.suggest_int('num_filters', input_width, 3*input_width)\n",
        "\n",
        "  multi_conv_model = cnn_model(num_filters, label_width, conv_width, 1, trial.suggest_categorical('activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh']))\n",
        "\n",
        "  multi_window = WindowGenerator(input_width=input_width,\n",
        "                               label_width=label_width,\n",
        "                               shift=label_width, \n",
        "                               train_df=weeklyTrainBuffer, \n",
        "                               test_df=weeklyTestBuffer, \n",
        "                               label_columns=[\"Units Sold\"])\n",
        "\n",
        "  history = compile_and_fit(multi_conv_model, multi_window)\n",
        "\n",
        "  return math.sqrt(multi_conv_model.evaluate(multi_window.test, verbose=0, return_dict=True)[\"loss\"])"
      ],
      "metadata": {
        "id": "NBGJ3TVqYRRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly1DConvModels = {}\n",
        "train_time = 3600\n",
        "for product in dailyData_train_clear[\"Product\"].unique():\n",
        "  weeklyTrainBuffer = weeklyData_train_clear[weeklyData_train_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  weeklyTestBuffer = weeklyData_test_clear[weeklyData_test_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='1D_CNN_Weekly')\n",
        "  study.optimize(objectiveWeekly1DConv, timeout=train_time, show_progress_bar=True)\n",
        "  trial = study.best_trial\n",
        "  weekly1DConvModels[product] = trial.params"
      ],
      "metadata": {
        "id": "7sdDSf8SYRVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(weekly1DConvModels, index=weekly1DConvModels[product].keys())"
      ],
      "metadata": {
        "id": "6yqQ5-d1Yp6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь приступаем к обучению LSTM сетей для обномерного случая"
      ],
      "metadata": {
        "id": "wRZBZY_wdQ6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model(num_units, label_width, num_features, activation, rec_activation):\n",
        "  \n",
        "  model = tf.keras.Sequential([\n",
        "    # Shape [batch, time, features] => [batch, lstm_units].\n",
        "    # Adding more `lstm_units` just overfits more quickly.\n",
        "    tf.keras.layers.LSTM(num_units, return_sequences=False, activation=activation, recurrent_activation=rec_activation),\n",
        "    # Shape => [batch, out_steps*features].\n",
        "    tf.keras.layers.Dense(label_width*num_features, kernel_initializer=tf.initializers.zeros()),\n",
        "    # Shape => [batch, out_steps, features].\n",
        "    tf.keras.layers.Reshape([label_width, num_features])\n",
        "])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "jZFTMKfeYp82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Реккурентная нейронная сеть\n",
        "def objectiveDaily1DLSTM(trial):\n",
        "  #Горизонт прогнозирования в окне - 3 недели\n",
        "  label_width = 21\n",
        "  num_units = trial.suggest_int('num_units', label_width, 3*label_width)\n",
        "\n",
        "  activation = trial.suggest_categorical('activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'])\n",
        "  rec_activation = trial.suggest_categorical('rec_activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'])\n",
        "\n",
        "  model = lstm_model(num_units, label_width, 1, activation, rec_activation)\n",
        "\n",
        "  window = WindowGenerator(input_width=label_width,\n",
        "                               label_width=label_width,\n",
        "                               shift=label_width, \n",
        "                               train_df=dailyTrainBuffer, \n",
        "                               test_df=dailyTestBuffer, \n",
        "                               label_columns=[\"Units Sold\"])\n",
        "\n",
        "  history = compile_and_fit(model, window)\n",
        "\n",
        "  loss = model.evaluate(window.test, verbose=0, return_dict=True)\n",
        "\n",
        "  return math.sqrt(model.evaluate(window.test, verbose=0, return_dict=True)[\"loss\"])"
      ],
      "metadata": {
        "id": "nJJp-GBPYp-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily1DLSTMModels = {}\n",
        "train_time = 3600\n",
        "for product in dailyData_train_clear[\"Product\"].unique():\n",
        "  dailyTrainBuffer = dailyData_train_clear[dailyData_train_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  dailyTestBuffer = dailyData_test_clear[dailyData_test_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='1D_LSTM_Daily')\n",
        "  study.optimize(objectiveDaily1DLSTM, timeout=train_time, show_progress_bar=True)\n",
        "  trial = study.best_trial\n",
        "  daily1DLSTMModels[product] = trial.params"
      ],
      "metadata": {
        "id": "SrxuPMNkYqBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(daily1DLSTMModels, index=daily1DLSTMModels[product].keys())"
      ],
      "metadata": {
        "id": "5dxKCf4ljMm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objectiveWeekly1DLSTM(trial):\n",
        "  #Горизонт прогнозирования в окне - 3 недели\n",
        "  label_width = 3\n",
        "  num_units = trial.suggest_int('num_units', label_width, 3*label_width)\n",
        "\n",
        "  activation = trial.suggest_categorical('activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'])\n",
        "  rec_activation = trial.suggest_categorical('rec_activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'])\n",
        "\n",
        "  model = lstm_model(num_units, label_width, 1, activation, rec_activation)\n",
        "\n",
        "  window = WindowGenerator(input_width=label_width,\n",
        "                               label_width=label_width,\n",
        "                               shift=label_width, \n",
        "                               train_df=weeklyTrainBuffer, \n",
        "                               test_df=weeklyTestBuffer, \n",
        "                               label_columns=[\"Units Sold\"])\n",
        "\n",
        "  history = compile_and_fit(model, window)\n",
        "\n",
        "  return math.sqrt(model.evaluate(window.test, verbose=0, return_dict=True)[\"loss\"])"
      ],
      "metadata": {
        "id": "Kcou8qbAjMrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly1DLSTMModels = {}\n",
        "train_time = 3600\n",
        "for product in dailyData_train_clear[\"Product\"].unique():\n",
        "  weeklyTrainBuffer = weeklyData_train_clear[weeklyData_train_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  weeklyTestBuffer = weeklyData_test_clear[weeklyData_test_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='1D_LSTM_Weekly')\n",
        "  study.optimize(objectiveDaily1DLSTM, timeout=train_time, show_progress_bar=True)\n",
        "  trial = study.best_trial\n",
        "  weekly1DLSTMModels[product] = trial.params"
      ],
      "metadata": {
        "id": "iLEIvGzkjMtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(weekly1DLSTMModels, index=weekly1DLSTMModels[product].keys())"
      ],
      "metadata": {
        "id": "g3g8drLejMwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь приступаем к обучению авторегрессионных LSTM сетей для обномерного случая\n",
        "\n"
      ],
      "metadata": {
        "id": "S7yUEQuym9aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedBack(tf.keras.Model):\n",
        "  def __init__(self, num_units, label_width, num_features, activation, rec_activation):\n",
        "    super().__init__()\n",
        "    self.label_width = label_width\n",
        "    self.num_units = num_units\n",
        "    self.lstm_cell = tf.keras.layers.LSTMCell(num_units, activation=activation, recurrent_activation=rec_activation)\n",
        "    # Also wrap the LSTMCell in an RNN to simplify the `warmup` method.\n",
        "    self.lstm_rnn = tf.keras.layers.RNN(self.lstm_cell, return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(num_features)\n",
        "  \n",
        "  def warmup(self, inputs):\n",
        "    # inputs.shape => (batch, time, features)\n",
        "    # x.shape => (batch, lstm_units)\n",
        "    x, *state = self.lstm_rnn(inputs)\n",
        "\n",
        "    # predictions.shape => (batch, features)\n",
        "    prediction = self.dense(x)\n",
        "    return prediction, state\n",
        "  \n",
        "  def call(self, inputs, training=None):\n",
        "    # Use a TensorArray to capture dynamically unrolled outputs.\n",
        "    predictions = []\n",
        "    # Initialize the LSTM state.\n",
        "    prediction, state = self.warmup(inputs)\n",
        "\n",
        "    # Insert the first prediction.\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    # Run the rest of the prediction steps.\n",
        "    for n in range(1, self.label_width):\n",
        "      # Use the last prediction as input.\n",
        "      x = prediction\n",
        "      # Execute one lstm step.\n",
        "      x, state = self.lstm_cell(x, states=state,\n",
        "                                training=training)\n",
        "      # Convert the lstm output to a prediction.\n",
        "      prediction = self.dense(x)\n",
        "      # Add the prediction to the output.\n",
        "      predictions.append(prediction)\n",
        "\n",
        "    # predictions.shape => (time, batch, features)\n",
        "    predictions = tf.stack(predictions)\n",
        "    # predictions.shape => (batch, time, features)\n",
        "    predictions = tf.transpose(predictions, [1, 0, 2])\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "cbjqrPopjMx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Реккурентная авторегрессионная нейронная сеть\n",
        "def objectiveDaily1DAutoregLSTM(trial):\n",
        "  #Горизонт прогнозирования в окне - 3 недели\n",
        "  label_width = 21\n",
        "  num_units = trial.suggest_int('num_units', label_width, 3*label_width)\n",
        "\n",
        "  activation = trial.suggest_categorical('activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'])\n",
        "  rec_activation = trial.suggest_categorical('rec_activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'])\n",
        "\n",
        "  model = FeedBack(num_units, label_width, 1, activation, rec_activation)\n",
        "\n",
        "  window = WindowGenerator(input_width=label_width,\n",
        "                               label_width=label_width,\n",
        "                               shift=label_width, \n",
        "                               train_df=dailyTrainBuffer, \n",
        "                               test_df=dailyTestBuffer, \n",
        "                               label_columns=[\"Units Sold\"])\n",
        "\n",
        "  history = compile_and_fit(model, window)\n",
        "\n",
        "  loss = model.evaluate(window.test, verbose=0, return_dict=True)\n",
        "\n",
        "  return math.sqrt(model.evaluate(window.test, verbose=0, return_dict=True)[\"loss\"])"
      ],
      "metadata": {
        "id": "vWOnzCZmjMz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "daily1DAutoregLSTMModels = {}\n",
        "train_time = 3600\n",
        "for product in dailyData_train_clear[\"Product\"].unique():\n",
        "  dailyTrainBuffer = dailyData_train_clear[dailyData_train_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  dailyTestBuffer = dailyData_test_clear[dailyData_test_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='1D_Autoreg_LSTM_Daily')\n",
        "  study.optimize(objectiveDaily1DAutoregLSTM, timeout=train_time, show_progress_bar=True)\n",
        "  trial = study.best_trial\n",
        "  daily1DAutoregLSTMModels[product] = trial.params"
      ],
      "metadata": {
        "id": "en9MJ_wPsFV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(daily1DAutoregLSTMModels, index=daily1DAutoregLSTMModels[product].keys())"
      ],
      "metadata": {
        "id": "yDbuLpxMsFX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objectiveWeekly1DAutoregLSTM(trial):\n",
        "  #Горизонт прогнозирования в окне - 3 недели\n",
        "  label_width = 3\n",
        "  num_units = trial.suggest_int('num_units', label_width, 3*label_width)\n",
        "\n",
        "  activation = trial.suggest_categorical('activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'])\n",
        "  rec_activation = trial.suggest_categorical('rec_activation', ['relu', 'elu', 'selu', 'gelu', 'sigmoid', 'tanh'])\n",
        "\n",
        "  model = FeedBack(num_units, label_width, 1, activation, rec_activation)\n",
        "\n",
        "  window = WindowGenerator(input_width=label_width,\n",
        "                               label_width=label_width,\n",
        "                               shift=label_width, \n",
        "                               train_df=weeklyTrainBuffer, \n",
        "                               test_df=weeklyTestBuffer, \n",
        "                               label_columns=[\"Units Sold\"])\n",
        "\n",
        "  history = compile_and_fit(model, window)\n",
        "\n",
        "  return math.sqrt(model.evaluate(window.test, verbose=0, return_dict=True)[\"loss\"])"
      ],
      "metadata": {
        "id": "WlIqGG9xsFaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly1DAutoregLSTMModels = {}\n",
        "train_time = 3600\n",
        "for product in dailyData_train_clear[\"Product\"].unique():\n",
        "  weeklyTrainBuffer = weeklyData_train_clear[weeklyData_train_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  weeklyTestBuffer = weeklyData_test_clear[weeklyData_test_clear[\"Product\"] == product].drop([\"index\", \"Product\"], axis=1)\n",
        "  study = optuna.create_study(direction='minimize', sampler=TPESampler(), study_name='1D_Autoreg_LSTM_Weekly')\n",
        "  study.optimize(objectiveWeekly1DAutoregLSTM, timeout=train_time, show_progress_bar=True)\n",
        "  trial = study.best_trial\n",
        "  weekly1DAutoregLSTMModels[product] = trial.params"
      ],
      "metadata": {
        "id": "3HI3FXgJsFed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(weekly1DAutoregLSTMModels, index=weekly1DAutoregLSTMModels[product].keys())"
      ],
      "metadata": {
        "id": "N8q6V-_lsFgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение нейросетей для одномерных случаев завершено."
      ],
      "metadata": {
        "id": "-AidT--l-gza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ссылки:\n",
        "\n",
        "1.   https://www.tensorflow.org/tutorials/structured_data/time_series?hl=ru\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kDwheO9LvscC"
      }
    }
  ]
}